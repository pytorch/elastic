# Python CircleCI 2.0 configuration file
#
# Check https://circleci.com/docs/2.0/language-python/ for more details
#
version: 2

# ------------------------------------------------------------------------------
# Test envs
# ------------------------------------------------------------------------------
cpu: &cpu
  docker:
  machine:
    image: default
  resource_class: medium

gpu: &gpu
  environment:
    CUDA_VERSION: "10.1"
  machine:
    image: default
  resource_class: gpu.medium  # Tesla M60

# ------------------------------------------------------------------------------
# test setup commands
# ------------------------------------------------------------------------------
setup_venv: &setup_venv
  - run:
      name: Activate Venv
      command: |
        python -m venv ~/venv
        echo ". ~/venv/bin/activate" >> $BASH_ENV
        . ~/venv/bin/activate
        python --version
        which python
        which pip

install_python: &install_python
  - run:
      name: Install Python
      working_directory: ~/
      command: |
        pyenv install 3.6.1
        pyenv global 3.6.1

install_dep: &install_dep
  - run:
      name: Install Dependencies
      command: |
        which python
        which pip
        pip install --upgrade pip
        # Remove manual install of torch nightly after torch 1.5.0 releases
        # https://github.com/pytorch/elastic/issues/39
        pip uninstall -y torch
        if [[ -z ${CUDA_VERSION} ]]; then
          pip install --progress-bar off --pre torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html
        else
          pip install --progress-bar off --pre torch -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html
        fi
        pip install --progress-bar off -r requirements.txt
        pip list

install_cuda: &install_cuda
  - run:
      name: Install CUDA
      working_directory: ~/
      command: |
        # download and install nvidia drivers, cuda, etc
        wget -q 'https://s3.amazonaws.com/ossci-linux/nvidia_driver/NVIDIA-Linux-x86_64-430.40.run'
        sudo /bin/bash ./NVIDIA-Linux-x86_64-430.40.run -s --no-drm
        wget -q https://developer.download.nvidia.com/compute/cuda/10.1/Prod/local_installers/cuda-repo-ubuntu1404-10-1-local-10.1.243-418.87.00_1.0-1_amd64.deb
        sudo dpkg -i cuda-repo-ubuntu1404-10-1-local-10.1.243-418.87.00_1.0-1_amd64.deb
        sudo apt-key add /var/cuda-repo-10-1-local-10.1.243-418.87.00/7fa2af80.pub
        nvidia-smi

install_etcd: &install_etcd
  - run:
      name: Install etcd
      command: |
        # pulls etcd server binary for spinning up a standalong etcd server for unittests
        # see test/p2p/etcd_server_fixture.py
        # needs to run AFTER checkout since it uses the install_etcd script in the repo
        echo "current directory is $(pwd)"
        echo "etcd install directory is: $ETCD_INSTALL_DIR"
        examples/bin/install_etcd -d $ETCD_INSTALL_DIR
        $ETCD_INSTALL_DIR/etcd --version
        $ETCD_INSTALL_DIR/etcdctl version

run_tests: &run_tests
  - run:
      name: Run Tests
      command: |
        python setup.py test -s test.suites.unittests -v

# ------------------------------------------------------------------------------
# tests
# ------------------------------------------------------------------------------
jobs:
  linter:
    <<: *cpu
    working_directory: ~/torchelastic
    steps:
      - checkout

      - <<: *install_python

      - run:
            name: setup lint
            command: |
                pip install --upgrade pip
                pip install --progress-bar off black isort

      - run:
            name: run isort and black
            command: |
              bash ./scripts/formatter_python.sh

  cpu_tests:
    <<: *cpu

    environment:
        ETCD_INSTALL_DIR: /tmp/etcd
        TORCHELASTIC_ETCD_BINARY_PATH: /tmp/etcd/etcd

    working_directory: ~/torchelastic

    steps:
      - checkout

      - <<: *install_etcd

      - <<: *install_python

      - <<: *setup_venv

      # Download and cache dependencies
      - restore_cache:
          keys:
            - v2-cpu-dependencies-{{ checksum "requirements.txt" }}
            # fallback to using the latest cache if no exact match is found
            - v2-cpu-dependencies-

      - <<: *install_dep

      - save_cache:
          paths:
            - ~/venv
          key: v2-cpu-dependencies-{{ checksum "requirements.txt" }}

      - <<: *run_tests


  gpu_tests:
    <<: *gpu

    working_directory: ~/torchelastic

    steps:
      - checkout

      - <<: *install_cuda

      - <<: *install_python

      - <<: *setup_venv

      # Download and cache dependencies
      - restore_cache:
          keys:
            - v1-gpu-dependencies-{{ checksum "requirements.txt" }}
            # fallback to using the latest cache if no exact match is found
            - v1-gpu-dependencies-

      - <<: *install_dep

      - run:
          name: Check CUDA Available
          command: python -c "import torch; assert torch.cuda.is_available(), 'CUDA not available'"

      - save_cache:
          paths:
            - ~/venv
          key: v1-gpu-dependencies-{{ checksum "requirements.txt" }}

      - <<: *run_tests

# -------------------------------------------------------------------------------------
# Workflows
# -------------------------------------------------------------------------------------
workflows:
  version: 2
  build_and_test:
    jobs:
      - linter
      - cpu_tests
      # no gpu tests for now; uncomment to enable
      # - gpu_tests
